{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Smote\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EDA with pandas-profiling\n",
    "import ydata_profiling as pdp\n",
    "\n",
    "# Monitoring\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Miscellaneous\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Regex\n",
    "import re\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"titanic/train.csv\")\n",
    "\n",
    "# # Generate a profiling report\n",
    "# profile = pdp.ProfileReport(df, title=\"Pandas Profiling Report\")\n",
    "# profile.to_file(\"pandas_profiling_report.html\")  # Save report as HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv(\"titanic/train.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Drop the specified columns from the DataFrame\n",
    "df = df.drop(columns=['PassengerId', \n",
    "                      'Ticket',  \n",
    "                      'Cabin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of missing values in each column\n",
    "missing_count = df.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values in each column\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "# Combine the count and percentage into a DataFrame\n",
    "df_missing = pd.DataFrame({\n",
    "    'Missing Count': missing_count,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "# Filter the DataFrame to include only columns with missing percentage greater than 0\n",
    "df_missing = df_missing[df_missing['Missing Percentage'] > 0]\n",
    "\n",
    "# Optionally, sort the DataFrame by the number of missing values (descending order)\n",
    "df_missing = df_missing.sort_values(by='Missing Count', ascending=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_missing)\n",
    "\n",
    "\n",
    "# Fill missing values in 'Embarked' with the most frequent value\n",
    "most_frequent_embarked = df['Embarked'].mode()[0]\n",
    "df['Embarked'].fillna(most_frequent_embarked, inplace=True)\n",
    "\n",
    "# Fill missing values in 'Age' with the median value\n",
    "median_age = df['Age'].median()\n",
    "df['Age'].fillna(median_age, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns from the DataFrame\n",
    "numeric_columns = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate the skewness for each numeric column in the DataFrame\n",
    "skewness_values = numeric_columns.skew()\n",
    "\n",
    "# Create a DataFrame to store the skewness values\n",
    "df_skew = pd.DataFrame({\n",
    "    'Column': skewness_values.index,\n",
    "    'Skewness': skewness_values.values\n",
    "})\n",
    "\n",
    "# Optionally, sort the DataFrame by skewness in descending order\n",
    "df_skew = df_skew.sort_values(by='Skewness', ascending=False)\n",
    "\n",
    "# Display the DataFrame with skewness values\n",
    "print(\"Skewness of numeric columns:\")\n",
    "print(df_skew)\n",
    "\n",
    "# Define columns for which to apply capping based on high skewness\n",
    "columns_to_cap = df_skew[df_skew['Skewness'] > 1]['Column']\n",
    "\n",
    "# Apply upper capping at the 95th percentile for columns with high skewness\n",
    "for col in columns_to_cap:\n",
    "    upper_cap = df[col].quantile(0.95)  # Calculate the 95th percentile\n",
    "    df[col] = np.where(df[col] > upper_cap, upper_cap, df[col])  # Cap values above the 95th percentile\n",
    "\n",
    "# Display the new skewness values after capping\n",
    "print(\"\\nNew skewness after capping:\")\n",
    "print(df[columns_to_cap].skew())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df[df.duplicated()]\n",
    "\n",
    "# Display the duplicate rows\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicate_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive regex pattern to search for a variety of titles\n",
    "title_pattern = r'\\b(Dr|Prof|Ph\\.D\\.|M\\.Sc\\.|B\\.Sc\\.|M\\.A\\.|B\\.A\\.|MBA|MD|DDS|DVM|JD|LLD|Sir|Dame|Lord|Lady|Baron|Baroness|Rev\\.|Father|Sister|Capt|Col|Major|Lt|Sgt|Admiral|General|Eng\\.|Architect|Attorney)\\b'\n",
    "\n",
    "# Apply the pattern to the Name column to extract titles\n",
    "df['Academic Title'] = df['Name'].apply(lambda x: re.search(title_pattern, x))\n",
    "\n",
    "# Create a new column 'Title' where 1 indicates the presence of a title and 0 indicates no title\n",
    "df['Title'] = df['Academic Title'].apply(lambda x: 1 if x else 0)\n",
    "\n",
    "# Drop the temporary 'Academic Title' column if it's no longer needed\n",
    "df = df.drop(columns=['Academic Title', 'Name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FamilySize feature\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1  # Adding 1 to include the passenger themselves\n",
    "\n",
    "# Display the first few rows to verify the new feature\n",
    "print(df[['SibSp', 'Parch', 'FamilySize']].head())\n",
    "\n",
    "\n",
    "# df = df.drop(columns=['SibSp', 'Parch'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical to Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only categorical columns from the DataFrame\n",
    "df['Pclass'] = df['Pclass'].astype('category')\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Display the names of all categorical columns\n",
    "print(\"Categorical Variables:\")\n",
    "print(categorical_columns)\n",
    "\n",
    "# Label encode the 'Sex' column\n",
    "label_encoder = LabelEncoder()\n",
    "df['Sex'] = label_encoder.fit_transform(df['Sex'])\n",
    "\n",
    "# One-hot encode the 'Pclass' and 'Embarked' columns\n",
    "df = pd.get_dummies(df, columns=['Pclass', 'Embarked'], drop_first=False)\n",
    "\n",
    "\n",
    "# Step 1: Convert all boolean columns to integers (0 and 1)\n",
    "df = df.applymap(lambda x: 1 if x is True else (0 if x is False else x))\n",
    "\n",
    "# Step 2: Ensure all columns are numeric\n",
    "df = df.apply(pd.to_numeric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalization / Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numeric columns to standardize\n",
    "numeric_columns = ['Age', 'Fare', 'FamilySize']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the numeric columns and transform them\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'Survived' is the target variable\n",
    "X = df.drop(columns=['Survived'])  # Features\n",
    "y = df['Survived']  # Target\n",
    "\n",
    "\n",
    "print(\"Original class distribution:\", Counter(y))\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"Class distribution after SMOTE:\", Counter(y_train_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initalizing Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),  # Increased max_iter for convergence\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(probability=True),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    'LightGBM': lgb.LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the results\n",
    "results_list = []\n",
    "\n",
    "# Perform cross-validation for each model\n",
    "for model_name, model in tqdm(models.items(), desc=\"Running models\"):\n",
    "    # Perform cross-validation for multiple metrics in one run\n",
    "    cv_results = cross_validate(model, X, y, cv=5, scoring=['accuracy', 'roc_auc', 'f1'])\n",
    "    \n",
    "    # Append results to the list\n",
    "    results_list.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy Mean': cv_results['test_accuracy'].mean(),\n",
    "        'Accuracy Std': cv_results['test_accuracy'].std(),\n",
    "        'ROC AUC Mean': cv_results['test_roc_auc'].mean(),\n",
    "        'ROC AUC Std': cv_results['test_roc_auc'].std(),\n",
    "        'F1 Score Mean': cv_results['test_f1'].mean(),\n",
    "        'F1 Score Std': cv_results['test_f1'].std()\n",
    "    })\n",
    "\n",
    "# Convert the results list into a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for SVM\n",
    "grid_search_svm = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_svm.fit(X, y)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(\"Best cross-validation accuracy for SVM:\", grid_search_svm.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for Gradient Boosting\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for Gradient Boosting\n",
    "grid_search_gb = GridSearchCV(GradientBoostingClassifier(), param_grid_gb, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_gb.fit(X, y)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters for Gradient Boosting:\", grid_search_gb.best_params_)\n",
    "print(\"Best cross-validation accuracy for Gradient Boosting:\", grid_search_gb.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for XGBoost\n",
    "grid_search_xgb = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'), param_grid_xgb, cv=5, scoring='accuracy', verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_xgb.fit(X, y)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "print(\"Best cross-validation accuracy for XGBoost:\", grid_search_xgb.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LightGMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for LightGBM\n",
    "param_grid_lgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'num_leaves': [31, 40, 50],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object for LightGBM\n",
    "grid_search_lgb = GridSearchCV(\n",
    "    lgb.LGBMClassifier(), \n",
    "    param_grid_lgb, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit the model with GridSearchCV\n",
    "grid_search_lgb.fit(X, y)\n",
    "\n",
    "# Output the best parameters and best cross-validation accuracy\n",
    "print(\"Best parameters for LightGBM:\", grid_search_lgb.best_params_)\n",
    "print(\"Best cross-validation accuracy for LightGBM:\", grid_search_lgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer with Batch Normalization and Dropout\n",
    "model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Second hidden layer with Batch Normalization and Dropout\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Third hidden layer with Batch Normalization and Dropout\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=2)\n",
    "\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_val = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_val, y_pred_val)\n",
    "roc_auc = roc_auc_score(y_val, y_pred_val)\n",
    "f1 = f1_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Validation ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Validation F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
